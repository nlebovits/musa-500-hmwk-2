---
title: 'MUSA 500, Assignment #2'
author: "Minwook Kang, Nissim Lebovits, Ann Zhang"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    code_folding: hide
    theme: darkly
    highlight: monochrome
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, cache = T, messages = F, warning = F, error = F)
```

# Introduction

## Problem & Setting

As in [our previous homework
assignment](https://rpubs.com/nlebovits/musa-550-assign-1-ed-2), this
study aims to examine the relationship between median house values and
several neighborhood characteristics. It will establish a model for
predicting median house values, with a geographic focus on Philadelphia.
Earlier models of house value prediction---especially the influential
HUD report, *Characteristic Prices of Housing in Fifty-Nine Metropolitan
Areas*---propose a hedonic model, which has been widely adopted and
which we will use here.[^1]

[^1]: Stephen Malpezzi, Larry Ozanne, and Thomas Thibodaeu,
    Characteristic Prices of Housing in Fifty-nine Metropolitan Areas
    (Washington D.C.: Department of Housing and Urban Development:
    1980).

$$R = f (S, N, C, t)$$ where

$R =$ rent or value,

$S =$ structural characteristics,

$N =$ neighborhood characteristics (including location),

$C =$ contract conditions (implicit and explicit), and

$t =$ time trend (accounting for inflation over time).

In a subsequent study that looks at Place-to-place Housing Price Indexes
and their determinants, the researchers utilized the log-linear form of
the hedonic model,[^2] expressed as:

[^2]: Stephen Malpezzi, Gregory H. Chun, and Richard K. Green, "New
    Place-to-Place Housing Price Indexes for U.S. Metropolitan Areas,
    and Their Determinants," Real Estate Economics 26, No. 2 (1998):
    235-274.
    <https://doi-org.proxy.library.upenn.edu/10.1111/1540-6229.00745>

$$\ln R = \beta_0 + S\beta_1 + N\beta_2 + L\beta_3 + C\beta_4 + \epsilon $$
where

$\ln R =$ the natural log of rent,

$R, S, N, C =$ same as above,

$\beta_i =$ the hedonic regression coefficient, and

$\epsilon$ = the residuals.

\
Since the researchers mentioned "four appealing characteristics" of the
log-linear form, including mitigating a common form of
heteroskedasticity,[^3] our study will also use the logarithmic
transformation of some variables.

[^3]: Malpezzi, Chun, and Green, "Place-to-Place Housing Price Index,"
    238.

\
Both studies identified and categorized structural, neighborhood, and
contract characteristics as three separate groups. Nonetheless, the
analysis primarily focused on the structural characteristics (e.g., the
number of bedrooms) and oversimplified the variables in neighborhood
characteristics (i.e., using a generic subjective rating of how "good" a
neighborhood is instead of using more objective parameters). Given that
previous studies largely focused on structural characteristics, we will
adopt similar regression models to explore four neighborhood
characteristics that were previously overlooked:

1)  the percentage of individuals with Bachelor's degrees or higher,

2)  the percentage of vacant houses,

3)  the number of households living in poverty, and

4)  the percentage of single family housing units.

In [our previous homework
assignment](https://rpubs.com/nlebovits/musa-550-assign-1-ed-2), we
found that the map of standardized residuals showed signs of spatial
autocorrelation, which undermined the viability of our OLS regression
model. In this assignment, we attempt to account for the influence of
spatial autocorrelation by by using various spatial regression
techniques, including spatial lag, spatial error, and geographically
weighted regression.

## Prior Analysis

In Homework #1, we utilized OLS (ordinary least sqaures) regression to
build a model looking at the relationship between a dependent variable
(log-transformed median house values) and four predictors (percentage of
vacant houses, percentage of single family houses, percentage of
residents holding a Bachelor's degree or higher, and log-transformed
poverty rate).

## Issues with OLS

Although the result of our OLS regression model was relatively
acceptable for social science research, it inevitably suffered from OLS
regression's inability to address the spatial component of our dataset.
Given the geographic nature of house price data, taking saptial elements
into consideration may substantially improve our model.

## Improving on OLS

This project aims to incorporate the spatial components of our data by
considering spatial lag, spatial error, and geographically weighted
regression (GWR). In the results and discussion sections, we will
revisit the OLS approach from last project and make comparisons between
different approaches, and suggest whether spatial lag, spatial error,
and/or GWR perform better than OLS.

# Methods

## Spatial Autocorrelation

Spatial autocorrelation looks at how the value of a variable for an
observation varies in comparison to values for other observations
nearby. In other words, it is the relationship of values of a single
variable at neighboring locations. Positive spatial autocorrelation
suggests more similar values at proximate locations, while negative
spatial autocorrelation suggests greater differences in the value of the
variable for more proximate observations.

### Tobler's 1st Law

Spatial autocorrelation is based on Waldo Tobler's First Law of
Geography. Proposed in 1970, Tobler's First Law states that, "everything
is related to everything else, but near things are more related than
distant things." Because of this potential association between values of
a variable and their locations, spatial statistics and autocorrelation
should be taken into consideration in models.

### Moran's I

One of the most commonly used tests for spatial autocorrelation (also
known as spatial dependencies) is Moran's I. Larger positive values of
Moran's I indicate a stronger positive spatial autocorrelation
(geospatial clustering of similar values), while larger negative values
indicate a strong negative correlation (spatial dispersion of similar
values). Value closer to 0 indicate little to no spatial
autocorrelation, i.e., random distribution.

**Present and explain formula for Moran's I**

### Weight Matrices

Weight matrices are a $n * n$ table (given $n$ observations in the
dataset) that is useful for calculating spatial autocorrelation indices
(which means, quantifying the extent of spatial autocorrelation). There
are different types of weight matrices, with "rook" and "queen" being
the two most frequently used contiguity-based spatial weights. Often,
statisticians try several different weight matrices to ensure that their
results are universal, unless they have theoretical reasoning that
strongly suggest the application of a particular weight matrix.

In this project, we are using a queen weight matrix, which accounts for
neighbors that are directly next to and diagonally linked to the area of
interest. In comparison to a rook weight matrix, which does not account
for diagonal areas (only four directions reflected, up, down, left, and
right), queen weights do a better job reflecting neighboring values in
all eight directions.

### Significance Testing

To test the significance of our our spatial autocorrelation tests
(Moran's I), we are using a Monte Carlo approach. This process requires
us to shuffle the value of the dependent variable 999 times, recalculate
the Moran's I value for each permutation, then rank and compare the
original Moran's I value with all 999 permutations. We calculate a
pseudo p-value by taking the rank of the original Moran's I compared to
the permutations and dividing it by 1,000. The pseudo p-value helps us
test the significance of our Moran's I and test our hypotheses, which
are:

$H_0$ = there is no spatial autocorrelation (i.e., the pattern is
random)

$H_a$ = there is spatial autocorrelation, whether positive or negative

### Local vs. Global Spatial Autocorrelation

Developed by Luc Anselin, Local Indices of Spatial Autocorrelation
(LISA) are used for calculating local spatial autocorrleation. Unlike
the global Moran's I, which generates one value, LISAs generates a local
Moran's I value for each location. While the global Moran's I helps us
understand the entire dataset, the local Moran's I values help us dig
into specific locations and see how they compare to their neighbors. Our
project calculates local Moran's I values in order to carry out further
analysis of spatial autocorrelation.

## OLS Regression and Assumptions

### Overview of OLS Regression

OLS (Ordinary least squares) is a method to explore relationships
between a dependent variable and one or more explanatory variables. It
considers the strength and direction of these relationships and the
goodness of model fit. (For more information, please refer to [our
previous homework
assignment](https://rpubs.com/nlebovits/musa-550-assign-1-ed-2).) When
running an OLS regression, it is crucial to ensure that the following
five assumptions are not violated:

1.  A linear relationship between the dependent variable and predictors
2.  Normality of residuals
3.  Homoscedasticity
4.  Independence of observations (no spatial, temporal or other forms of
    dependence in the data)
5.  No multicollinearity

In Homework #1, we showed that three of above five OLS assumptions were
violated. First, the relationships between our dependent variable and
the predictor variables were generally not linear. Additionally, the
scatterplot of predicted values compared to standardized residuals
showed heteroscedasticity. Finally, our map of the standardized
residuals showed signs of spatial autocorrelation, indicating systematic
patterns in our data, violating the fourth assumption. These three
issues undermine key assumptions of linear regression.

### Random Errors in Spatial Data

When residuals contain a systematic pattern, the assumption of
randomness of residuals is not met. Specifically, if we have spatially
autocorrelated OLS residuals, there is systematic under-prediction or
over-prediction in certain parts of the study region; furthermore, the
significance estimates for the $\beta$ coefficients in OLS may be
incorrect (inflated). There are a couple ways to check for spatial
autocorrelation of the OLS residuals, though the first thing to do might
be to map the residuals. We want the map of the residuals to look
random; this would imply that our model is not spatially autocorrelated.
After looking at the residual maps, we would want to compute the global
Moran's I of the residuals. Ideally, we will see a Moran's I close to 0,
there is an obvious problem with spatial autocorrelation of residuals.

Below in [section 3.2.2](#scatterplot-of-residuals), we will regress
residuals $\hat{ε}$ on the spatially lagged residuals $W\hat{ε}$.
Ideally, we would see that there is no relationship between $\hat{ε}$
and $W\hat{ε}$ -- that is, that the coefficient of $W\hat{ε}$, which we
denote as $\beta_1$, is not significantly different from 0. $\beta_1$
can range between -1 and 1. We are able to regress residuals on the
nearest neighbor residuals, thereby filtering the spatial information
out of the OLS residuals and decomposing the residuals $ε$ into two
parts: one with a spatial pattern $λWε$ and one which is simply random
noise $u$. $$ y = β_0 + β_1X_1 +...+ β_nX_n +λWε+ u$$

### Testing Other Regression Assumptions in R

Below, in [section 3.2](#ols-output) and [section
3.3](#spatial-lag-regression-results), we will test the null hypothesis
of homoscedasticity against the alternative hypothesis of
heteroscedasticity using the following tests:

1)  Koenker-Bassett Test : Robust Tests for Heteroscedasticity Based on
    Regression Quantiles
2)  Breusch-Pagan Test : Simple test for heteroscedasticity and random
    coefficient variation
3)  White Test : A Heteroskedasticity-Consistent Covariance Matrix
    Estimator and a Direct Test for Heteroskedasticity.

For these tests, $H_0$ is the assumption of homoscedasticity, while
$H_a$ is the presence of heteroscedasticity. If the p-value \< 0.05, we
can reject $H_0$.

Another important assumption is the normality of residuals. When
residuals are not normal, we may run into issues with both OLS
regression and spatial regression. We can check the normality of
residuals by:

1)  Examining the histogram of residuals, and
2)  Running the Jarque-Bera test.

Here, $H_0$ is the normality of residuals, while $H_a$ is the non-normal
distribution of residuals, If the p-value \< 0.05, we can reject $H_0$.

## Spatial Lag and Spatial Error Regression

### Spatial Lag Regression

The spatial lag model assumes that the value of the dependent variable
at one location is associated with the values of that variable in nearby
locations. "Nearby" is as defined by the weights matrix W (rook, queen,
or within a certain distance of one another). In other words, the
spatial lag model includes the spatial lag of the dependent variable as
a predictor. In this assignment, our equation for the Spatial Lag Model
is as follow:
$$ LNMEDHVAL = ρWy\;+\;β_0\;+\;LNNBELPOVβ_1\;+\;PCTBACHMORβ_2\;+\;PCTSINGLESβ_3\;+\;PCTVACANTβ_4\;+\;ε $$
Here, $ρ$ is the coefficient of the $y-lag$ variable $Wy$, just as $β_1$
is the coefficient of the variable $LNNBELPOV$, and $ε$ is the residual
term.

It may be help to consider spatial lag through the analogy of this Stats
Wizard Penguin (SWP) and his Inept Companions (ICs): although the SWP
may have a stats wizardry quotient (SWQ) of over 9,000, its spatial lag
(i.e., the average value of the SWQs of its neighboring ICs) is 0
because all the neighboring penguins know exactly 0 about statistics.
(Crucially, we should use a symmetrical matrix of statistically
incompetent penguins, as this is a key requirement of spatial lag and
spatial error regressions).

```{r tweet embed, echo = F}
library(tweetrmd)

tweetrmd::tweet_embed("https://twitter.com/schumacherbj/status/1584278266546130944",
                      widget_type = "video")
```

### Spatial Error Regression

The spatial error model assumes that the *residual* at one location is
associated with *residuals* at nearby locations. "Nearby" is as defined
by the weights matrix W (rook, queen, or within a certain distance of
one another). This requires two steps: First, we run our OLS regression,
where we regress Y on the predictors.
$$ LNMEDHVAL = β_0\;+\;LNNBELPOVβ_1\;+\;PCTBACHMORβ_2\;+\;PCTSINGLESβ_3\;+\;PCTVACANTβ_4\;+\;ε $$
Second, we regress the residuals on the nearest neighbor residuals,
thereby filtering the spatial information out of the OLS residuals and
decomposing the residuals $ε$ into two parts, one with a spatial pattern
$λWε$ and one which is simply random noise $u$.

$$ ε  = λWε\;+\; u$$ The part with a spatial pattern can be thought of
as some variable with a spatial component missing from the first OLS
regression.
$$ LNMEDHVAL = β_0\;+\;LNNBELPOVβ_1\;+\;PCTBACHMORβ_2\;+\;PCTSINGLESβ_3\;+\;PCTVACANTβ_4\;+\; λWε\;+\; u $$

### Assumptions for Spatial Lag and Error Regression

With both spatial lag and spatial error regression models, we still
retain all of the assumptions of OLS *except for the spatial
independence of observations.* The relationship between the independent
variable and each of the dependent variables must still be, residuals
must still be linear, and there should be no multicollinearity.

### Goals of Spatial Lag and Error Regression

The goal of spatial regression is to consider the possibility of spatial
dependency in the residuals. By using this method, we want to remove
spatially autocorrelation from our model and make our model less
heteroscedastic.

### Regression Performance Compared

Various measures can be used to compare models. The first one is the
Akaike Information Criterion and the Schwartz Criterion. These measure
the goodness of fit of an estimated statistical model. They are relative
measures of the information that is lost when a given model is used to
describe reality, and they can be said to describe the trade-off between
precision and complexity of the model. The equation of the AIC is the
following: $$AIC = 2k - 2ln(\hat{L})$$ Given a set of candidate models
for the data, the preferred model is the one with the lowest AIC value.
Thus, the AIC rewards goodness of fit (as assessed by the likelihood
function), but it also includes a penalty that is an increasing function
of the number of estimated parameters. The penalty discourages
overfitting, which is desired because increasing the number of
parameters in the model almost always improves the goodness of fit.

The second option is the log likelihood, which is associated with the
maximum likelihood method of fitting a statistical model to the data and
estimating model parameters. The maximum likelihood approach picks the
values of the model parameters that make the data more likely than any
other values of the parameters would make them. Thus, the higher the log
likelihood, the better the model fit. This comparison should be used for
nested models only. OLS is a special case of spatial lag and spatial
error models. So we can compare the OLS model with the spatial lag and
spatial error models, but we cannot use log likelihood to compare the
spatial lag and spatial error models to each other because they are not
a special case of each other.

A likelihood ratio test is another tool to compare the OLS model to the
spatial model. $H_0$ for this test is that the spatial model is not a
better specification than the OLS model. If the p-value \< 0.05, we
reject $H_0$ and state that the spatial lag (error) model is better than
the OLS model.

We can use Moran's I to figure out which model is better between the OLS
and spatial lag and spatial error models. First, we run OLS regression
and examine the residuals for spatial autocorrelation (Moran's I). If
there is no significant spatial autocorrelation in the residuals, we
stop. If there is significant spatial autocorrelation in residuals, we
need to look at the the Lagrange Multiplier (lag) and the Lagrange
Multiplier (error), which tells us whether we should run a spatial
regression or spatial error model. If neither Lagrange Multiplier is
significant, then the OLS model is good enough. If the Lagrange
Multiplier for the spatial lag model is significant and the Lagrange
Multiplier for the spatial error model is not, then we run the spatial
lag model. If the Lagrange Multiplier for the spatial error model is
significant and Lagrange Multiplier for the spatial lag model is not,
then we run the spatial error model. If both are significant (as in this
assignment), we look at we look at the Robust LM for both the spatial
lag and spatial error models. We choose the model which has a lower
p-value or higher test statistic value and run that model.

## Geographically Weighted Regression

### Simpson's Paradox and Local Regression

Simpson's Paradox is a statistical phenomenon where an association
between two variables in a population emerges, disappears or reverses
when the population is divided into sub-populations. OLS regression,
spatial lag regression, and spatial error regression assume spatial
stationarity, i.e., that modeled relationships are constant across space. In
spatial statistics, stationarity equals the homogeneity of an effect,
or, that a process works the same regardless of where you observe the
process; it is a necessary assumption, but likely not true in practice.
For example, higher home values might be correlated with higher crime
rates in some locations and lower crime rates in some others. The aim of
geographically weighted regression is to separate area into multiple
locations and conducting multiple local regression instead of having a
single, global regression.

### GWR Equations

OLS models are run to determine the global regression coefficients($β$)
for the independent variables is following:
$$y_i = β_0 + \displaystyle\sum_{k=1}^{n}{β_kx_{ik}} + ε_i$$

where $y_i$ is the $ith$ observation of the dependent variable, $x_{ik}$
is the $ith$ observation of the $kth$ independent variable, the $ε_i$s
are independent normally distributed error terms with zero means, and
each $β_k$ must be determined from a sample of $n$ observations. Usually
the least squares method is used to estimate the $β_n$s. Using matrix
notation this may be expressed as follows:
$$\hat{β} = (x^tx)^{-1}x^ty$$. where the independent observations are
the columns of $x$ and the dependent observations are the single column
vector $y$. The column vector $\hat{β}$ contains the coefficient
estimates. Each of these estimates can be thought of as a "rate of
change" between one of the independent variables and the dependent
variable.

Once the independent variables that you wish to retain in the model are
identified, and there is a theoretical basis for thinking that the
relationships may differ by space, GWR may be an appropriate next step. GWR is a relatively simple
technique that extends the traditional regression framework
by allowing local variations in rates of change so that the coefficients
in the model are specific to a
location $i$, rather than being global estimates. The regression equation is then:

$$y_i = β_{i0} + \displaystyle\sum_{k=1}^{n}{β_{ik}x_{ik}} + ε_i$$ where
$β_{ik}$ is the value of the $kth$ parameter at location $i$. Using a
weighted least squares approach to calibrating regression models,
different emphases can be placed on different observations in generating
the estimated parameters. In ordinary least squares, the sum of the
squared differences of predicted and actual $y_i$s is minimized by the
coefficient estimates. In weighted least squares a weighting factor $w$,
is applied to each squared difference before minimizing, so that the
inaccuracy of some predictions carries more of a penalty than others. If
$w$ is the diagonal matrix of $w_i$s. then the estimated coefficients
satisfy $$\hat{β} = (x^twx)^{-1}x^twy$$ In GWR, weighting an observation
in accordance with its proximity to $i$ would allow an estimation of
$β_{ik}$ to be made that meets the criterion of "closeness of
calibration points" set out above. Note that usually in weighted
regression models the values of $w_i$ are constant, so that only one
calibration has to be carried out to obtain a set of coefficient
estimates, but in this case $w$ varies with $i$ so that a different
calibration exists for every point in the study area. In this case, the
parameter estimation formula could be written more generally as
$$\hat{β}_i = (x^tw_ix)^{-1}x^tw_iy$$

### Running Local Regression

We need multiple locations to run a regression, not just a
single location. GWR uses other observations in the dataset to run the
regression. Observations that are close to location $i$ are given
greater weights. The weight of an observation varies with proximity to location $i$.
Normally, observations that are closer to location $i$ have a stronger
influence on the estimation of the parameters for location $i$.

An initial step toward weighting based on locality might be to exclude
from the model calibration observations that are further than some
distance $d$ from the locality. This would be equivalent to setting their
weights to zero, giving a weighting function of
$$w_{ij} = 1 \;\;\;\;\;\;\; if \;\;d_{ij} < d\\w_{ij} = 0 \;\;\;\;\;\;\; otherwise$$
However, the spatial weighting function above suffers from
discontinuity. As $i$ varies around the study area, the regression
coefficients could change drastically as one sample point moves into or
out of the circular buffer around $i$ and which defines the data to be
included in the calibration for location $i$.

### Bandwidth (Adaptive vs. Fixed)

There are a couple of ways to weight nearby locations considering the
problem of discontinuity. Graphically, GWR involves fitting a spatial
kernel to the data as described in the Figure 1. For a given regression
point $X$, the weight $W$ of a data point is at a maximum at the location
of the regression point. The weight decreases gradually as the distance
between two points increases. A regression model is thus calibrated
locally by moving the regression point across the area under study. For
each location, the data are weighted differently so that the resulting
estimates are unique to a particular location. ![Figure 1. GWR with
fixed spatial kernel. Source: Fotheringham et al.
(2002)](https://github.com/nlebovits/musa-500-hmwk-2/raw/main/img/fixed_bandwidth.png)
With fixed bandwidth $h$, $w_{ij}$ is as follows:
$$w_{ij} = \begin{cases}
e^{-0.5(\frac{distance_{ij}}{h})^2},\;\;\;\;\;if\;\;\;distance_{ij}\leq h\\
\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;0,\;\;\;\;\;if\;\;\;otherwise
\end{cases}$$ $Distance_{ij}$ is the distance between the regression
point $i$ and data point $j$, and number of observations will vary
around each point $i$, but the bandwidth distance $h$ will remain
constant.

A key issue is to decide between two options of spatial kernels: a fixed
kernel or an adaptive kernel. Intuitively, a fixed kernel involves using
a fixed bandwidth to define a region around all regression points as
displayed in Figure 2. The extent of the kernel is determined by the
distance to a given regression point, with the kernel being identical at
any point in space. An adaptive kernel involves using varying bandwidth
to define a region around regression points as displayed in Figure 2.
The extent of the kernel is determined by the number of nearest
neighbors from a given regression point. The kernels have larger
bandwidths where the data are sparse. ![Figure 2. GWR with adaptive
spatial kernel. Source: Fotheringham et al.
(2002)](https://github.com/nlebovits/musa-500-hmwk-2/raw/main/img/adaptive_bandwidth.png)
With adaptive bandwidth $h$, $w_{ij}$ is as follows :
$$w_{ij} = \begin{cases}
\Bigl[1-(\frac{distance_{ij}}{h})^2\Bigr]^2,\;\;\;\;\;if\;\;j\;\;is\;one\;of\;i's\;N\;nearest\;neighbors\\
\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;0,\;\;\;\;\;if\;\;otherwise
\end{cases}$$ $Distance_{ij}$ is the distance between the regression
point $i$ and data point $j$, and number of observations will remain
constant around each point $i$, but the bandwidth distance $h$ will
vary. As for us, adaptive bandwidth is more appropriate because our data
varies across space.

### Assumptions in GWR

Many of the assumptions in OLS--such as the normality of residuals, homoscedastiticty,
the absence of multicollinearity, and the normal distribution of residuals--still
need to be met for GWR. GWR builds a local regression equation for each feature in the dataset.
When the value for an explanatory varially clusters spatially in a substantial way,
we are still likely to have a problem. That is, the variable tells the same story
at every location. We also may have a problem with multicollinearity when we have two or
more variables that have similar patterns of clusters in a certain
region. Likewise, We may run into problems if we use two variables which both have
very high (or very low) values at all locations in a certain part of the
study region. The condition number in the attribute table indicates when
results are unstable due to local multicollinearity. To check this
issue, we can look through the Cond.Number field from the GWR results
attribute table, As a rule of thumb, 
multicollinearity issues likely arise for features with a condition number larger
than 30, equal to Null, or equal to -1.7976931348623158e + 308.

### GWR and P-Values

In a global model it is usual to test whether the parameter estimates
are significantly different from zero. This can be accomplished with a
t-test. The situation with GWR is a little more complex and is the
subject of current research. As there is one set of parameters
associated with each regression point, as well as one set of standard
errors, there are potentially thousands of tests that would be
required to determine whether parameters are locally significant. The
assumption behind the tests means that if the 0.05 significance level is
used, we would expect 5 tests in every 100 to be significant. With a
5 variable model estimated at 20,000 regression points, we would expect
5,000 of these tests to return a significant result. This is the problem
raised by multiple testing.

## Tools

In our assignment, we rely on the usual `tidyverse`, `sf`, and `tmap` packages in R. We also incorporate a number of spatial packages such as `sp`, `rgdal`, and `rgeos`. Most of our spatial regression work relies on `spdep` and `spgwr`. Importantly, we use the [recently-released `sfdep` package](https://sfdep.josiahparry.com/), which is basically a `tidy` version of the `spdep` package and allows us to incorporate `sf` objects. Finally, we use `lmtest`, `whitestrap`, and `tseries` for the Breusch-Pagan, White, and Jarque-Bera tests respectively.

```{r, include=F}

# install.packages(c("sp", "rgdal", "rgeos", "spdep", "spgwr", "spatialreg", "GWmodel"))
#install.packages("remotes")
#remotes::install_github("gadenbuie/tweetrmd")

#remove.packages("rlang")
#install.packages("rlang")

library(tidyverse) #general
library(sf) #spatial
library(mapview) #quick mapping
library(tmap) #full mapping
library(ggpubr) #for ggarrange
library(gt) #for tables
library(glue) #for tables
library(janitor) #to clean col names
library(corrplot) #for easy correlation matrix
library(tmap) #for choropleth maps
library(MASS) #for stepwise regression
library(DAAG) #for CVlm
library(caret) #for a different attempt at cvlm
library(stargazer) # for cleaner reg tables
library(sp) # for spatial
library(rgdal) # for spatial
library(rgeos) # for spatial
library(spdep) #for spatial lag + error reg
library(spgwr) # for geographically weighted reg
library(spatialreg) #for spatial lag + error reg (alt)
library(GWmodel) # for geographically weighted reg (alt)
library(tweetrmd) # to embed a tweet
library(sfdep) # for sf replacement of spdep
library(lmtest) #for breusch pagan test
library(whitestrap) # for white test
library(tseries) # for jarque bera test
```

## Import and Prepare Data

As in Homework #1, we are completing this assignment in R (rather than
using GeoDa and ArcGIS). Once again, we will read in the original
shapefile of data and transform the relevant columns to produce the
initial data that we need for our analysis.

```{r import}
#reg_data = st_read('C:/Users/Nissim/Desktop/Fall 2022/Spat Stats/musa-500-hmwk-2/reg_data_files/Regression Data.shp')

#Min
reg_data = read_sf('C:/Users/Nissim/Desktop/Fall 2022/Spat Stats/musa-500-hmwk-2/data/Regression Data.shp')

#define a function to find zero values in columns

# This function applies log transformations to the relevant columns. It checks whether there are zero values in each column and then applies the appropriate log transformation accordingly.

col_zeros = function(a, b) {
                  pct_col_zeros = count(subset(st_drop_geometry(a), b != 0)) |>
                                      pull(n) / nrow(st_drop_geometry(a))
                  return(pct_col_zeros)
                  }
#apply function with case_when statement
#case_when is a vectorized function, while ifelse is not.
#running this with ifelse will result in all row values in the mutated column being the same.
reg_data = reg_data |>
            mutate(
                ln_med_h_val = case_when(col_zeros(reg_data, reg_data$MEDHVAL) == 1 ~ log(reg_data$MEDHVAL),
                                     TRUE ~ log(1 + reg_data$MEDHVAL)),
                   ln_pct_bach_more = case_when(col_zeros(reg_data, reg_data$PCTBACHMOR) == 1 ~ log(reg_data$PCTBACHMOR),
                                     TRUE ~ log(1 + reg_data$PCTBACHMOR)),
                   ln_n_bel_pov_100 = case_when(col_zeros(reg_data, reg_data$NBelPov100) == 1 ~ log(reg_data$NBelPov100),
                                     TRUE ~ log(1 + reg_data$NBelPov100)),
                   ln_pct_vacant = case_when(col_zeros(reg_data, reg_data$PCTVACANT) == 1 ~ log(reg_data$PCTVACANT),
                                     TRUE ~ log(1 + reg_data$PCTVACANT)),
                   ln_pct_singles = case_when(col_zeros(reg_data, reg_data$PCTSINGLES) == 1 ~ log(reg_data$PCTSINGLES),
                                     TRUE ~ log(1 + reg_data$PCTSINGLES)),
                  )
```

# Results

## Spatial Autocorrelation

### Distribution of Median House Value

The map below displays the spatial distribution of median house value by
census tract in Philadelphia. As a starting point, this map offers us an overview of (log of) median house values in the city. We can observe a clustering of higher values in the north-east and north-west of the city and a clustering of lower values in the center to north part. Statistical tests are applied to further explore such phenomena and results are presented as following. 

```{r medhval choro}
tmap_mode("plot")

tm_shape(reg_data) + 
  tm_polygons(title = "ln(MEDHVAL)", col = "LNMEDHVAL", border.col = NA, border.alpha = 0, lwd = 0, palette = "viridis", style = "jenks") + 
 # tm_shape(phl_city_lims) +
#  tm_borders(col = "grey", lwd = 5) +
  tm_compass(position = c("left", "top")) +
  tm_layout(main.title = "Median House Values by Tract",
            legend.position = c("right", "bottom"),
            frame = FALSE)
```

### Global Moran's I

Below, we calculate a global Moran's I value and analyze it using a Monte Carlo approach,
which compares our calculated Moran's I to 999 random permutations.

First, we'll prepare a weight matrix based on Queen neighbors.[^4]

[^4]: This analysis is based on [Data Analysis and Visualization with
    R:Spatial (2022)](http://www.geo.hunter.cuny.edu/~ssun/R-Spatial/)

```{r queen weights}

# create our queen weight matrix
# reg_data_nb = reg_data |>
        #              spdep::poly2nb(row.names = reg_data$POLY_ID)

# grab geometry
reg_data = reg_data |>
              mutate(nb_geom = st_geometry(reg_data), #geoms for poly2nb
                     nb = st_contiguity(nb_geom), # generate neighbors
                     weights = st_weights(nb)) # weight matrices from neighbors

summary(reg_data$nb)
```

We can visualize the connections between neighbors based on the weight
matrix that we've connected:

```{r global moran map}
reg_data_lines = nb2lines(reg_data$nb, 
                          coords = st_centroid(reg_data$geometry), 
                          as_sf = TRUE)

tm_shape(reg_data) + 
  tm_borders(col = "grey", lwd = 0.5) + 
tm_shape(reg_data) +
  tm_dots() +
tm_shape(reg_data_lines) +
  tm_lines(col = "red") +
tm_layout(frame = FALSE)
```

Now we can apply the weight matrix that to calculate the global Moran's
I.

```{r global moran}
# Global Moran's I

# now use that weight matrix to calculate global moran's i for MEDHVAL

global_moran(reg_data$ln_med_h_val, reg_data$nb, reg_data$weights)$`I`
```

Having calculated a global Moran's I, we'll use a Monte Carlo approach
to test whether our global Moran's I is statistically significant. If
the null hypothesis is true, then the probability of drawing the
observed data is the same as any other permutation of the $i$'s amongst
the polygons. Thus, if $m$ is the number of simulated Moran's I values
exceeding the observed one, and $M$ is the total number of simulations,
then the probability of getting the observed Moran's I or greater is

$$p = \frac{m+1}{M+1}$$

```{r global moran test}

# reshuffling test w 999 permutations
# present Moran's I value, histogram of Moran's I values with all permutations, and p-value

moranMC = global_moran_perm(reg_data$ln_med_h_val, reg_data$nb, reg_data$weights, alternative = "two.sided", 999)

moranMC
```

Here we visualize the randomly permuted values from the Monte Carlo
test.

```{r global moran hist}
#draws a histogram of our randomly permuted values
# and adds a red line for our actual moran's i

moranMCres = moranMC$res |>
              as.data.frame()

colnames(moranMCres) = "col1"

ggplot(moranMCres) +
  geom_histogram(aes(col1), bins = 100) +
  geom_vline(xintercept = moranMC$statistic, col = "red") +
  labs(title = "Histogram of MoranMCres",
       x = "moranMCres",
       y = "Frequency") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

Additional confirmation of the significance of our Moran's I value can
be found by plotting the relationship between ln_med_h_val of the block
groups and their neighbors. If there were no relationship between block
group observations and those of their neighbors, we would not see a
clear pattern in the plot below. However, we observe that this is not
the case.

```{r global moran plot}
moran.plot(reg_data$ln_med_h_val, nb2listw(reg_data$nb),
           xlab = "ln_med_h_val", 
            ylab = "Spatial Lag")
```

### Local Moran's I

The `local_moran` function in `sfdep` allows us to compute LISA
statistics for each block group. This function generates the Local Moran
statistic as `ii`. We might also be interested in `var_ii`, which tells
us how much each block group's Local Moran Statistic varies from the
global mean, and `p_ii`, which indicates whether the Local Moran
Statistic can be considered statistically significant.

```{r local moran}
# Local Moran's I

# local Moran's I analysis with queen weight matrix; print results

lisa = local_moran(reg_data$ln_med_h_val, reg_data$nb, reg_data$weights, nsim = 999)

head(lisa)
```

Now we can examine the local Moran's I values to see if there is
significant spatial autocorrelation. Mapping the p-values that we've just
calculated, it appears that we have statistically significant (p \< 0.05)
clustering in parts of northwest, northeast, north, and west Philadelphia.

```{r local moran map}
# first we need to bind the lisa dataframe to our reg_data dataframe

reg_data = cbind(reg_data, lisa)

tm_shape(reg_data) +
  tm_fill(style = "fixed", 
          col = "p_ii", 
          breaks = c(0,0.001, 0.01, 0.05, 1), 
          palette = '-viridis',
          title = "P-Value") +
  #tm_borders(col = "white",
   #          lwd = 0.05) +
  tm_layout(frame = FALSE, main.title = "P-Value")
```

We can now visualize statistically significant clustering throughout the city,
which the local Moran's I calculation groups into High-High, High-Low, Low-High,
and Low-Low clusters. (We will also exclude clusters for which p \> 0.05.)

Based on our map, it appears that there are significant High-High clusters in
northwest and northeast Philadelphia, as well as some small neighborhoods near
Old City, along City Line Avenue, and near the Navy Yard in south Philadelphia.
Significant Low-Low clusters are also found in North Philadelphia, parts of West
Philadelphia, and small scattered sites near Grays Ferry.

```{r local moran clustering}
reg_data = reg_data |>
            mutate(
                  mean = as.character(mean),
                  lisa_map = case_when(
                                  p_ii < 0.05 ~ mean,
                                  TRUE ~ "Insignificant"
                                )
                  )

pal = c("#B2182B", "#F4A582", "lightgrey", "#92C5DE", "#2166AC")

tm_shape(reg_data) +
  tm_fill(col = "lisa_map", palette = pal, title = "LISA Clusters")+
  tm_borders(col = "white", lwd = 0.05) +
  tm_layout(frame = FALSE, main.title = "LISA Clusters")

```

## OLS Regression and Assumptions

### OLS Output {#ols-output}

Our model regressed the log of median house values (ln_med_h_val) on four predictors, namely,

 1) the percents of individual with bachelor’s degrees or higher (PCBACHMORE),
 2) the number of households living below the poverty line (LNNBELPOV),
 3) the percent of vacant houses (PCTVACANT), and
 4) the percent of single houses units (PCTSINGLES).

The output of our OLS regression indicates that all four predictors are statistically significant; the p-value for the F-statistic is less than 0.05, as are the individual p-values for each predictor. Additionally, our OLS model explains more than 60% of the variance of LNMEDHVAL ($R^2$ is 0.6623 and the Adjusted $R^2$ is 0.6615). LNNBELPOV and PCTVACANT are negatively associated with LNMEDHVAL, while PCTBACHMOR and PCTSINGLES are positively associated with LNMEDHVAL.

```{r ols}
# OLS Regression

reg = lm(formula=ln_med_h_val ~ ln_n_bel_pov_100 + PCTBACHMOR + PCTSINGLES + PCTVACANT, data = reg_data)

summary(reg)
```

In order to compare our OLS model to the other models generated later in this assignment, we will run a log likelihood test. We will return to these results later.

```{r log liklihood}
logLik(reg)
```

To test for heteroscedasticity, we will run three tests:

1)  the Breusch-Pagan test

```{r bp test}
bptest(reg, studentize = FALSE)
```

2)  the Koenker-Bassett test (also known as the Studentized
    Breusch-Pagan test)

```{r kb test}
bptest(reg, studentize = TRUE)
```

3)  the White test

```{r white test}
white_test(reg)
```

In all three cases, p \< 0.05, indicating a problem with heteroscedasticity in our data.

Finally, we will run the Jarque-Bera test to assess whether the
residuals of our regression are normal. Here, too, p \< 0.05, indicating that our residuals are not normally distributed.

```{r jb test}
jarque.bera.test(reg$residuals)
```

### Scatterplot of Residuals {#scatterplot-of-residuals}

Here, we'll generate standardized residuals, which are OLS Model
residuals divided by an estimate of their standard deviation. We can evaluate these standardized residuals to consider spatial autocorrelation in our OLS model by comparing them to their spatial lag.

First, we plot standardized residuals against the spatially lagged residuals. We see strong correlation, which is confirmed by the output of the linear model: $\rho$ (the slope of our linear model in the plot) is 0.732, indicating strong spatial autocorrelation in our OLS model.

```{r ols resids scatter}
reg_data = reg_data |>
              mutate(stand_resids = rstandard(reg),
                     spatial_lag = st_lag(stand_resids, 
                                             nb, 
                                             weights))

ggplot(reg_data, aes(x = spatial_lag, y = stand_resids)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = 'lm') +
  labs(x = "Spatial Lag of Residuals",
       y = "Standardized Residuals") +
  theme_minimal() +
  theme(aspect.ratio = 1)
```
```{r ols resids lm}
lm(formula =  stand_resids ~ spatial_lag, data = reg_data)
```

Further confirmation of this comes from mapping our standardized residuals, which appear to cluster in northwest, northeast, and north Philadelphia, as well as possibly downtown.

```{r ols resids}
tm_shape(reg_data)+
  tm_fill(col = 'stand_resids', 
          style = 'quantile', 
          title = 'Standardized OLS Residuals', 
          palette ='viridis',
          midpoint = 0)+
  tm_layout(frame = FALSE, 
            title = 'Standardised OLS Residuals')
```

However, a visual assessment is not sufficient, and we will test the
presence of spatial autocorrelation in two ways: 
 1) by regressing residuals on their queen neighbors, and 
 2) by looking at the Moran's I of the residuals.

First, let's regress the OLS standardized residuals on the spatial lag
of the OLS residuals (i.e., OLS residuals at the queen neighbors).

```{r reg resids on nn}
resids_lm = lm(formula = stand_resids ~ spatial_lag, data = reg_data)

summary(resids_lm)
```

### Moran's I Significance

Once again, we can use the Monte Carlo simulation to generate a Moran's
I statistics and a pseudo p-value.

```{r stand resids mc}
global_moran_perm(reg_data$stand_resids, reg_data$nb, reg_data$weights, alternative = "two.sided", 999)
```

It is strongly apparent that spatial autocorrelation exists among the
regression residuals of the OLS Model. The p-value is very small,
indicating that Moran's I is significant. Because there's clearly
spatial autocorrelation in our OLS residuals, the OLS Model is inappropriate
and we need to consider another method. Next, we will attempt to run the
spatial lag model, the spatial error model, and geographically weighted
regression.

```{r stand resids scatter}
moran.plot(reg_data$stand_resids, nb2listw(reg_data$nb),
           xlab = "Standardized Residuals", 
            ylab = "Spatial Lag")
```

## Spatial Lag and Spatial Error Regression

### Spatial Lag Regression Results {#spatial-lag-regression-results}

i.  Present results of Spatial Lag regression (call this Table 2)

<!-- -->

1.  Talk about the W_LNMEDHVAL term in the spatial lag regression
    output. State whether it is significant, and how the results can be
    interpreted.
2.  Are the remaining terms (i.e., the predictors LNNBELPOV, PCTBACHMOR,
    PCTSINGLES, and PCTVACANT) in the model significant?

<!-- -->

a.  Compare these results to OLS results.

<!-- -->

3.  State whether, based on the Breusch-Pagan test, the spatial lag
    regression residuals are still heteroscedastic.
4.  Compare the Spatial Lag regression and OLS regression models based
    on the Akaike Information Criterion/Schwarz Criterion, the Log
    Likelihood, and the Likelihood Ratio Test.

Here we have generated a spatial lag regression model and summarized the output. The coefficient of spatial lag
$\rho = 0.65$ and is significant (p \< 0.05), indicating that a one unit increase in the spatial lag of LNMEDHVAL
is associated with a 0.65 unit increase in LNMEDHVAL at a given location. Furthermore, as in our OLS model,
all four of our predictors have p \< 0.05, meaning that they are significant. 
```{r spatial lag}
spatial_lag_reg = lagsarlm(formula = ln_med_h_val ~ ln_n_bel_pov_100 + PCTBACHMOR + PCTSINGLES + PCTVACANT, data = reg_data, nb2listw(reg_data$nb))

sumslr = summary(spatial_lag_reg)

sumslr
```

To test for heteroscedasticity, we will once again run two of the three
tests run above. In both cases, p \< 0.05, indicating problems with heteroscedasticity in our spatial lag model.

1)  the Breusch-Pagan test

```{r bp test lag reg}
bptest.Sarlm(spatial_lag_reg, studentize = FALSE)
```

2)  the Koenker-Bassett test (also known as the Studentized
    Breusch-Pagan test)

```{r kb test lag reg}
bptest.Sarlm(spatial_lag_reg, studentize = TRUE)
```

Likewise, we again run the Jarque-Bera test to assess whether the residuals of our
regression are normal. Here, too, p \< 0.05, indicating an non-normality in the distribution of the residuals of our spatial lag model.

```{r jb test lag reg}
jarque.bera.test(spatial_lag_reg$residuals)
```

Next, we will consider spatial autocorrelation in our model. Visually, it appears that the residuals from our spatial lag model show less autocorrelation than in our OLS model.

```{r spatial lag stnad resids map}
reg_data = reg_data |>
              mutate(
                spatial_lag_resids = lagsarlm(formula = ln_med_h_val ~ ln_n_bel_pov_100 + PCTBACHMOR + PCTSINGLES + PCTVACANT, data = reg_data, nb2listw(reg_data$nb))$residuals
              )

tm_shape(reg_data)+
  tm_fill(col = 'spatial_lag_resids', 
          style = 'quantile', 
          title = 'Spatial Lag Residuals', 
          palette ='Blues',
          midpoint = 0)+
  tm_layout(frame = FALSE)
```

Running a Monte Carlo simulation for the global Moran's I of the spatial lag residuals, 

```{r spatial lag stand resids mc}
global_moran_perm(spatial_lag_reg$residuals, reg_data$nb, reg_data$weights, alternative = "two.sided", 999)
```

5.  Present the Moran's I scatterplot of spatial lag regression
    residuals. Does there seem to be less spatial autocorrelation in
    these residuals than in OLS residuals?
6.  Overall, which model is doing better based on all of these criteria?

```{r spatial lag moran}
# Moran's I for spatial lag model

# once again, run moran's I for the spatial lag residuals; test w 999 permutations
# print scatterplot & summary table of significance test
```

Lastly, we can compare our spatial lag model to the previous OLS model using both the log likelihood ratio and the AIC.

The log likelihood of our spatial lag model is higher, and the 

Additionally, using the likelihood ratio stest, we can see that the p-value \< 0.05, so we can reject $H_0$ and state that the spatial lag model is better than the OLS model.
```{r likelihood ratio}
LR.Sarlm(spatial_lag_reg, reg) #Here lagreg is the SL output; reg is the OLS output
```

```
```

```
stepAIC(reg)$anova$AIC,
        spatial_lag_reg$AIC_lm.model,
```

### Spatial Error Regression Results

Present results of Spatial Error regression (call this Table 3) 1. Talk
about the LAMBDA term in the spatial lag regression output. State
whether it is significant, and how the results can be interpreted. 2.
Are the remaining terms (i.e., the predictors LNNBELPOV, PCTBACHMOR,
PCTSINGLES, and PCTVACANT) in the model significant? a. Compare these
results to OLS results. 3. State whether, based on the Breusch-Pagan
test, the spatial lag regression residuals are still heteroscedastic? 4.
Compare the Spatial Error regression and OLS regression based on the
Akaike Information Criterion/Schwarz Criterion, the Log Likelihood, and
the Likelihood Ratio Test.

```{r spatial error}
spatial_error_reg = errorsarlm(formula = ln_med_h_val ~ ln_n_bel_pov_100 + PCTBACHMOR + PCTSINGLES + PCTVACANT, 
                               data = reg_data, 
                               nb2listw(reg_data$nb))

summary(spatial_lag_reg)
```

As with our OLS before, we will calculate the likelihood ratio for our
spatial lag model.

```{r error likelihood ratio}
LR.Sarlm(spatial_error_reg, reg) #Here lagreg is the SL output; reg is the OLS output
```

To test for heteroscedasticity, we will once again run two of the three
tests run above:

1)  the Breusch-Pagan test

```{r bp test error reg}
bptest.Sarlm(spatial_error_reg, studentize = FALSE)
```

2)  the Koenker-Bassett test (also known as the Studentized
    Breusch-Pagan test)

```{r kb test error reg}
bptest.Sarlm(spatial_error_reg, studentize = TRUE)
```

And, again, the Jarque-Bera test to assess whether the residuals of our
regression are normal.

```{r jb test error reg}
jarque.bera.test(spatial_error_reg$residuals)
```

```{r spatial error stnad resids map}
reg_data = reg_data |>
              mutate(
                spatial_error_resids = errorsarlm(formula = ln_med_h_val ~ ln_n_bel_pov_100 + PCTBACHMOR + PCTSINGLES + PCTVACANT, data = reg_data, nb2listw(reg_data$nb))$residuals
              )

tm_shape(reg_data)+
  tm_fill(col = 'spatial_error_resids', 
          style = 'quantile', 
          title = 'Spatial Error Residuals', 
          palette ='Blues',
          midpoint = 0)+
  tm_layout(frame = FALSE)
```

```{r spatial error stand resids mc}
global_moran_perm(spatial_error_reg$residuals, reg_data$nb, reg_data$weights, alternative = "two.sided", 999)
```

5.  Present the Moran's I scatterplot of spatial error regression
    residuals. Does there seem to be less spatial autocorrelation in
    these residuals than in OLS residuals?
6.  Overall, which model is doing better based on all of these criteria?

```{r spatial error moran}
# Moran's I for spatial error model

# once again, run moran's I for the spatial error residuals; test w 999 permutations
# print scatterplot & summary table of significance test
```

### Spatial Lag vs. Spatial Error

Compare the Spatial Lag and Spatial Error results with each other 1.
Recall that you should not be using the likelihood-ratio test for this
because the models are not nested (i.e., neither method is a special
subtype of each other). However, it is OK to compare the two non-nested
models, such as spatial lag and spatial error, based on Akaike
Information Criterion and the Schwarz Information Criterion. a. Which
model has better (lower) Akaike Information Criterion and Schwarz
Information Criterion values?

## Geographically Weighted Regression

### GWR Results

i.  Present GWR results from the \_supp table (call this Table 4)

<!-- -->

1.  Compare the (overall) R-squared of the GWR regression with the
    R-squared of the OLS regression. State which regression method seems
    to be doing a better job of explaining the variance in the dependent
    variable.
2.  Compare the Akaike Information Criteria of GWR with those of OLS,
    Spatial Lag and Spatial Error models. Which model seems to be doing
    a better job based on that (remember, the lower the Akaike
    Information Criterion, the better the fit).

Here we create an adaptive bandwidth.

```{r adaptive bandwidth est}
adapt_bandwidth_est = GWmodel::bw.gwr(formula = ln_med_h_val ~ ln_n_bel_pov_100 + PCTBACHMOR + PCTSINGLES + PCTVACANT, 
                                data = reg_data |> sf::as_Spatial(),
                                approach = 'AIC', 
                                kernel = 'gaussian', 
                                adaptive = TRUE)

adapt_bandwidth_est = gwr.sel(formula=ln_med_h_val ~ ln_n_bel_pov_100 + PCTBACHMOR + PCTSINGLES + PCTVACANT, 
                              data = reg_data |> sf::as_Spatial(),
                              method = "aic",
                              adapt = TRUE)
```

Here we create a fixed bandwidth.

```{r fixed bandwidth est}
fixed_bandwidth_est = gwr.sel(formula=ln_med_h_val ~ ln_n_bel_pov_100 + PCTBACHMOR + PCTSINGLES + PCTVACANT, 
                              data = reg_data |> sf::as_Spatial(),
                              method = "aic",
                              adapt = FALSE)

fixed_bandwidth_est
```

Here we run a basic GWR with an adaptive bandwidth.

```{r adaptive bw gwr}
gwrmodel_adaptive = gwr(formula = ln_med_h_val ~ ln_n_bel_pov_100 + PCTBACHMOR + PCTSINGLES + PCTVACANT,
                        data = reg_data |> sf::as_Spatial(),
                        adapt = adapt_bandwidth_est, #adaptive bandwidth determined by proportion of observations accounted for
                        gweight = gwr.Gauss,
                        se.fit = TRUE, #to return local standard errors
                        hatmatrix = TRUE)

gwrmodel_adaptive
```

Here we run a basic GWR with a fixed bandwidth.

```{r fixed bw gwr}
# Perform a basic GWR
gwrmodel_fixed = gwr(formula = ln_med_h_val ~ ln_n_bel_pov_100 + PCTBACHMOR + PCTSINGLES + PCTVACANT,
              data = reg_data |> sf::as_Spatial(),
              bandwidth = fixed_bandwidth_est, #adaptive bandwidth determined by proportion of observations accounted for
              gweight = gwr.Gauss,
              se.fit = TRUE, #to return local standard errors
              hatmatrix = TRUE)

gwrmodel_fixed
```

```{r gwr dfs}
gwr_adaptive_results = as.data.frame(gwrmodel_adaptive$SDF)

gwr_fixed_results = as.data.frame(gwrmodel_fixed$SDF)
```

We can look at a summary of the coefficients of the local regressions,
stored in the SDF object within the gwrmodel. Note in particular the
minimum and maximum values of the Local $R^2$
(`r min(gwr_adaptive_results$localR2)` -
`r max(gwr_adaptive_results$localR2)`). There are no negative values,
meaning that this output, unlike the output we get in some versions of
ArcGIS Pro, is correct.

### Local GWR Results

```{r local gwr summary}
summary(gwrmodel_adaptive$SDF)
```

We can also map the standardized coefficients. The higher the absolute
value of the ratio between the coefficient and the standard error, the
more plausible it is that the relationship between the predictor and the
dependent variable is significant at the location.

```{r gwr dataframe and map}
reg_data = reg_data |>
              mutate(
                coef_ln_n_bel_pov_100_adapt_stand = gwr_adaptive_results$ln_n_bel_pov_100 / gwr_adaptive_results$ln_n_bel_pov_100_se,
                coef_pctbachmor_adapt_stand = gwr_adaptive_results$PCTBACHMOR / gwr_adaptive_results$PCTBACHMOR_se,
                coef_pctsingles_adapt_stand = gwr_adaptive_results$PCTSINGLES / gwr_adaptive_results$PCTSINGLES_se,
                coef_pctvacant_adapt_stand = gwr_adaptive_results$PCTVACANT / gwr_adaptive_results$PCTVACANT_se,
                gwr_adapt_resids = gwr_adaptive_results$gwr.e,
                gwr_adapt_r2 = gwr_adaptive_results$localR2,
              )


coef_ln_n_bel_pov_100_adapt_stand_map = tm_shape(reg_data)+
                                    tm_fill(col='coef_ln_n_bel_pov_100_adapt_stand', 
                                            breaks=c(-Inf, -6, -4, -2, 0, 2, 4, 6, Inf), 
                                            title='Stand. coef. of ln_n_bel_pov_100', 
                                            palette ='-RdBu')+
                                    tm_layout(frame=FALSE, 
                                              main.title = '# Bel. Pov Line (Log)')

coef_pctbachmor_adapt_stand_map = tm_shape(reg_data)+
                                    tm_fill(col='coef_pctbachmor_adapt_stand', 
                                            breaks=c(-Inf, -6, -4, -2, 0, 2, 4, 6, Inf), 
                                            title='Stand. coef. of PCTBACHMOR', 
                                            palette='-RdBu')+
                                    tm_layout(frame=FALSE, 
                                              main.title = '% w/Bach/ Deg.+')

coef_pctsingles_adapt_stand_map = tm_shape(reg_data)+
                                    tm_fill(col='coef_pctsingles_adapt_stand', 
                                            breaks=c(-Inf, -6, -4, -2, 0, 2, 4, 6, Inf), 
                                            title='Stand. coef. of PCTSINGLES', 
                                            palette='-RdBu')+
                                    tm_layout(frame=FALSE, 
                                              main.title = '% Single Housing Units')

coef_pctvacant_adapt_stand_map = tm_shape(reg_data)+
                                    tm_fill(col='coef_pctvacant_adapt_stand', 
                                            breaks=c(-Inf, -6, -4, -2, 0, 2, 4, 6, Inf), 
                                            title='Stand. coef. of PCTVACANT', 
                                            palette='-RdBu')+
                                    tm_layout(frame=FALSE, 
                                              main.title = 'Percentage of Housing Vacant')


tmap_arrange(coef_ln_n_bel_pov_100_adapt_stand_map, 
             coef_pctbachmor_adapt_stand_map,
             ncol = 2)

tmap_arrange(coef_pctsingles_adapt_stand_map,
             coef_pctvacant_adapt_stand_map, 
             ncol=2)
```

### Local GWR R-Squared Results

```{r gwr rsquared}
tm_shape(reg_data)+
  tm_fill(col = 'gwr_adapt_r2',  
          breaks = c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8), 
          n = 5, 
          palette = 'Blues',
          title = "R Squared")+
  tm_layout(frame = FALSE, 
  main.title = 'Local R Squared')
```

### Local GWR Residuals Results

```{r gwr resids}
tm_shape(reg_data)+
  tm_fill(col = 'gwr_adapt_resids',  
          style = 'quantile',
          palette = 'Blues', 
          title = "Residuals",
          midpoint = 0)+
  tm_layout(frame = FALSE, 
  main.title = 'Residuals',
  title = "Adaptive GWR")
```

### Moran's I Scatterplot of GWR Residuals

ii. Present the Moran's I scatterplot of GWR residuals. Does there seem
    to be less spatial autocorrelation in these residuals than in OLS
    residuals? What about the Spatial Lag and Spatial Error Residuals.

```{r gwr moran}
moran.plot(reg_data$gwr_adapt_resids, nb2listw(reg_data$nb),
           xlab = "Standardized Residuals", 
            ylab = "GWR Residuals (Adaptive)")


```

## Model Comparison

We can do the following to compare all 4 models: 1. Look at the Moran's
I of the residuals from each model and choose the model with the lowest
(absolute) Moran's I 2. Compare the AIC values from each model and
choose the model with the lowest value In addition, we can do the
following to compare the Spatial Lag Model to OLS: 1. Compare the log
likelihoods; the model with the higher value is the better one. 2.
Examine the results of the likelihood ratio test; if it's significant,
the Spatial Lag Model is better than OLS. In addition, we can do the
following to compare the Spatial Error Model to OLS: 1. Compare the log
likelihoods; the model with the higher value is the better one. 2.
Examine the results of the likelihood ratio test; if it's significant,
the Spatial Error Model is better than OLS.

Note that the global R\^2 for our GWR here is not exactly the same as in
the actual function output; we're calculating it [based on the source
code](https://stackoverflow.com/questions/43927662/return-the-global-r2-of-a-geographically-weighted-regression-gwr-in-r)
so that it will be automatically update in the table.

```{r model comp}
model = c("OLS", "Spatial Lag", "Spatial Error", "GWR")
global_moran = c(global_moran_perm(reg$residuals, reg_data$nb, reg_data$weights, alternative = "two.sided", 999)$statistic,
                 global_moran_perm(spatial_lag_reg$residuals, reg_data$nb, reg_data$weights, alternative = "two.sided", 999)$statistic,
                 global_moran_perm(spatial_error_reg$residuals, reg_data$nb, reg_data$weights, alternative = "two.sided", 999)$statistic,
                 global_moran_perm(reg_data$gwr_adapt_resids, reg_data$nb, reg_data$weights, alternative = "two.sided", 999)$statistic)
  
aic = c(stepAIC(reg)$anova$AIC,
        spatial_lag_reg$AIC_lm.model, #there's a difference between AIC and AIC for the lm for both this and spatial error
        spatial_error_reg$AIC_lm.model, #not sure why that is--might have to ask Eugene
        #also, why are these both the same?? they shouldn't be...
        gwrmodel_adaptive$results$AICh)

log_lik = c(logLik(reg), logLik(spatial_lag_reg), NA, NA)

lik_ratio = c(NA, LR.Sarlm(spatial_lag_reg, reg)$statistic, LR.Sarlm(spatial_error_reg, reg)$statistic, NA)
  
lik_rat_p = c(NA, LR.Sarlm(spatial_lag_reg, reg)$p.value, LR.Sarlm(spatial_error_reg, reg)$p.value, NA)

r2 = c(summary(reg)$r.squared, NA, NA, (1 - (gwrmodel_adaptive$results$rss/gwrmodel_adaptive$gTSS))) #am I meant to be using r squared or ADJUSTED r squared?
                                            # calculating global r squared for gwr based on: https://stackoverflow.com/questions/43927662/return-the-global-r2-of-a-geographically-weighted-regression-gwr-in-r

comp = cbind(model,
             round(global_moran, 3),
             round(aic, 1),
             round(log_lik, 4),
             round(lik_ratio, 1),
             lik_rat_p,
             round(r2, 4)) |>
      as.data.frame()

colnames(comp) = c("Model", "Moran's I", "AIC", "Log Likelihood", "Likelihood Ratio", "L.R. P-Value", "R^2")

table_out = comp |>
        gt() |>
        tab_header(
          title = md("**Model Comparison**"))
     
#print output
table_out
```

Eugene says: "Lastly, we can also do the following to compare GWR to
OLS: 1. Compare the R2 from OLS with the Quasi-global R2 from GWR; the
model with the higher value is the better one. 2. Examine whether
there's spatial variability in standardized coefficients or local R2
values. If there is, it might mean that a single global regression might
not capture the varying spatial relationships between the dependent
variable and the predictors."

However, I don't think means that for #2 we can look at *local* $R^2$ or
coefficients, because I don't think those exist for an OLS model---only
for a spatial model. We should ask about this, though.

I also cannot figure out why that last map isn't rendering right. It
does in mapview!

```{r ols vs gwr map}
gwr_local_r2 = tm_shape(reg_data)+
                          tm_fill(col = 'gwr_adapt_r2', 
                                  style = 'quantile', 
                                  title = 'GWR Local R^2', 
                                  palette = 'Blues',
                                midpoint = 0)+
                          tm_layout(frame = FALSE)

gwr_coef_ln_n_bel_pov_100 = tm_shape(reg_data)+
                        tm_fill(col = 'coef_ln_n_bel_pov_100_adapt_stand', 
                                style = 'quantile', 
                                title = 'GWR Coef., ln_n_bel_pov_100', 
                                palette = 'Blues',
                                midpoint = 0)+
                        tm_layout(frame = FALSE)

gwr_coef_pctbachmor = tm_shape(reg_data)+
                        tm_fill(col = 'coef_pctbachmor_adapt_stand', 
                                style = 'quantile', 
                                title = 'GWR Coef., PCTBACHMOR', 
                                palette = 'Blues',
                                midpoint = 0)+
                        tm_layout(frame = FALSE)

gwr_coef_pctsingles = tm_shape(reg_data)+
                        tm_fill(col = 'coef_pctsingles_adapt_stand', 
                                style = 'quantile', 
                                title = 'GWR Coef., PCTSINGLES', 
                                palette = 'Blues',
                                midpoint = 0)+
                        tm_layout(frame = FALSE)


gwr_coef_pctvacant = tm_shape(reg_data)+
                        tm_fill(col = 'coef_pctvacant_adapt_stand', 
                                style = 'quantile', 
                                title = 'GWR Coef., PCTVACANT', 
                                palette = 'Blues',
                                midpoint = 0)+
                        tm_layout(frame = FALSE)

# mapview(reg_data, zcol = 'coef_pctvacant_adapt_stand')

gwr_local_r2

tmap_arrange(gwr_coef_ln_n_bel_pov_100,
             gwr_coef_pctbachmor,
             gwr_coef_pctsingles,
             gwr_coef_pctvacant,
             ncol = 2)
```

# Discussion

In this project, we used three approaches in creating model other than
OLS, including spatial lag, spatial error, and geographically weighted
regression (GWR), to check whether they can better account for spatial
autocorrelation than OLS.

Based on the results indicated in the 'model comparison' section above,
GWR is the best out of the four regression methods, not only because it
has a high $R^2$ value than OLS but also because it has an Moran's I
that is closer to 0 out of the four and lower AIC than spatial error or
spatial lag. Such results suggest that GWR is overall a better model
with better ability to account for spatial autocorrelation.

b)  Give a brief description of the limitations (i.e., which assumptions
    were not met).

# Citations

```{r citation lolz}
url = "https://www.smbc-comics.com/comics/1618931828-20210420.png"
```

<center><img src="`r url`"/></center>

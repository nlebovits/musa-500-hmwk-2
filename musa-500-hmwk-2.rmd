---
title: 'MUSA 500, Assignment #2'
author: "Minwook Kang, Nissim Lebovits, Ann Zhang"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    code_folding: hide
    theme: darkly
    highlight: monochrome
editor_options: 
  markdown: 
    wrap: 72
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, cache = T, messages = F, warning = F, error = F)
```

# Introduction

## Problem & Setting

As in [our previous homework assignment](https://rpubs.com/nlebovits/musa-550-assign-1-ed-2), this study aims to examine the relationship between median house values and several neighborhood characteristics. It will establish a model for predicting median house values, with a geographic focus on Philadelphia. Earlier models of house value prediction---especially the influential HUD report, *Characteristic Prices of Housing in Fifty-Nine Metropolitan Areas*---propose a hedonic model, which has been widely adopted and which we will use here.^[Stephen Malpezzi, Larry Ozanne, and Thomas Thibodaeu, Characteristic Prices of Housing in Fifty-nine Metropolitan Areas (Washington D.C.: Department of Housing and Urban Development: 1980).]

$$R = f (S, N, C, t)$$ where

$R =$ rent or value, 

$S =$ structural characteristics, 

$N =$ neighborhood characteristics (including location),

$C =$ contract conditions (implicit and explicit), and

$t =$ time trend (accounting for inflation over time).

In a subsequent study that looks at Place-to-place Housing Price Indexes
and their determinants, the researchers utilized the log-linear form of
the hedonic model,^[Stephen Malpezzi,  Gregory H. Chun, and Richard K. Green, “New Place-to-Place Housing Price Indexes for U.S. Metropolitan Areas, and Their Determinants,” Real Estate Economics 26, No. 2 (1998): 235-274. https://doi-org.proxy.library.upenn.edu/10.1111/1540-6229.00745] expressed as:

$$\ln R = \beta_0 + S\beta_1 + N\beta_2 + L\beta_3 + C\beta_4 + \epsilon $$ where

$\ln R =$ the natural log of rent, 

$R, S, N, C =$ same as above,

$\beta_i =$ the hedonic regression coefficient, and 

$\epsilon$ = the residuals.

\
Since the researchers mentioned "four appealing characteristics" of the log-linear form, including mitigating a common form of heteroskedasticity,^[Malpezzi, Chun, and Green, “Place-to-Place Housing Price Index,” 238.] our study will also use the logarithmic transformation of some variables. 

\
Both studies identified and categorized structural, neighborhood, and contract characteristics as three separate groups. Nonetheless, the analysis primarily focused on the structural characteristics (e.g., the number of bedrooms) and oversimplified the variables in neighborhood characteristics (i.e., using a generic subjective rating of how "good" a neighborhood is instead of using more objective parameters). Given that previous studies largely focused on structural characteristics, we will adopt similar regression models to explore four neighborhood characteristics that were previously overlooked: 

1)  the percentage of individuals with Bachelor's degrees or higher, 

2)  the percentage of vacant houses, 

3)  the number of households living in poverty, and 

4)  the percentage of single family housing units.

In [our previous homework assignment](https://rpubs.com/nlebovits/musa-550-assign-1-ed-2), we found that the map of standardized residuals showed signs of spatial autocorrelation, which undermined the viability of our OLS regression model. In this assignment, we attempt to account for the influence of spatial autocorrelation by by using various spatial regression techniques, including spatial lag, spatial error, and geographically weighted regression.

## Prior Analysis
In HW 1, we utilized OLS (ordinary least sqaures) regression to build a model looking at the relationship between a dependent variable (log-transformed median house values) and four predictors (percentage pf vacant house, percentage of single family house, percentage of residents holding a Bachelor or higher degree, and log-transformed poverty rate). 


## Issues with OLS
Although the result of our OLS regression model was relatively acceptable for social science research, it inevitably suffered from OLS regression's incapability dealing with the spatial component of our dataset . Given the geographic nature of house price data, taking sptial elements into consideration may largely improve our model. 

## Improving on OLS
Hence, this project aims to incorporate spatial components of our dataset via methods of spatial lag, spatial error, and geographically weighted regression (GWR). In the results and discussion sections, we will revisit the OLS approach from last project and make comparisons between different approaches, and suggest whether spatial lag and/or spatial error and/or GWR performs better than OLS. 


# Methods

## Spatial Autocorrelation 
Spatial autocorrelation looks at how one observation of a variable at one location vary from another observation at a nearby location, in other words, the relationship of values of a single variable at neighboring locations. A positive spatial autocorrelation suggests more similar values at locations that are closer by, while negative spatial autocorrelation suggests observations at closer locations show values that are more different. 

Spatial Autocorrelation is guided by and based on the premise of Tobler's 1st Law of Geography. 

### Tobler's 1st Law
As one of the most influential work, Waldo Tobler proposed the First Law of Geography in 1970. The law states that, "everything is related to everything else, but near things are more related than distant things." Because of this potential association between values of a variable and their locations, spatial statistics and autocorrelation should be taken into consideration into models. 

### Moran's I
Talk about Moran’s I; Present and explain formula for Moran’s I


Minwook Kang test

### Weight Matrices
Weight Matrices consists of a $n x n$ table (given n observations in the dataset). They are useful for calculating spatial auto-correlation

Mention and explain the weight matrix that you’re using.

1.	Indicate that throughout this report, you will be using this weight matrix.
2.	Specify why statisticians sometimes like to use more than one spatial weight matrix in their analyses.

Queen Weight Matrix 

### Significance Testing
In your own words, talk about how you test whether the spatial autocorrelation (Moran’s I) is significant. State what hypotheses you’re testing (present the null and alternative hypotheses) and describe the random permutation process.

### Local vs. Global Spatial Autocorrelation
Briefly describe the concept of local spatial autocorrelation, without going into any of the mathematical detail.


##-----------------------------min doing-------------------------------------------------------------------------------------------##

## OLS Regression and Assumptions

### Overview of OLS Regression
Begin by giving a brief (3-5 sentence) overview of OLS regression. Specifically, list the assumptions of OLS
1.	Refer the reader to your HW 1 for more information on OLS.
[FYI: Referring the reader to a previous HW assignment is often done in ESE 502 in order to avoid rewriting a lot of the same things over again]. 

lists of the OLS Regression's assumptions

1. the linear relationship between the dependent variable and predictors
2. the normality of residuals
3. homoscedasticity
4. the independence of observations(no spatial, temporal or other forms of dependence in data)
5. the absence of multicollinearity

According to the HW 1, three of above five OLS assumptions were violated. First, the relationships between our dependent variable and the predictor variables were generally not linear. Additionally, the scatterplot of predicted values compared to standardized residuals shows heteroscedasticity. Finally, our map of the standardized residuals shows signs of spatial autocorrelation, indicating systematic patterns in our data, which is violated fourth assumption. All three of these issues undermine key assumptions of linear regression. 

### Random Errors in Spatial Data

ii.	State that when the data has a spatial component, the assumption that your errors are random/independent often doesn’t hold
1.	Indicate that you can test the assumption in (ii) above by examining the spatial autocorrelation of the residuals using Moran’s I.
2.	Indicate that another way to test OLS residuals for spatial autocorrelation is to regress them on nearby residuals (here, these nearby residuals are residuals at neighboring block groups, as defined by the Queen matrix). 

When residuals contain a systematic pattern, the assumption of randomness of residuals is not met. Specifically, if we have spatially autocorrelated OLS residuals, there is systematic under-prediction or over-prediction in certain parts of the study region; furthermore, the significance estimates for the β coefficients in OLS may be incorrect (inflated). There are a couple ways to check for spatial autocorrelation of the OLS residuals, though the first thing to do might be to map the residuals. We want the map of the residuals to look random, and it implies that our model is not spatially autocorrelatedAfter looking at the residual maps, we would want to	Compute the Moran’s I of the residuals. Ideally, we will see a Moran’s I close to 0, there is an obvious problem with spatial autocorrelation of residuals.

2.	Indicate that another way to test OLS residuals for spatial autocorrelation is to regress them on nearby residuals (here, these nearby residuals are residuals at neighboring block groups, as defined by the Queen matrix). a.	Mention rho (ρ) and how it is calculated. [It’s that term that’s known as lambda (λ) in GeoDa, and is referred to as Slope b in the statistics at the bottom of the scatterplot of OLS_RESIDU and WT_RESIDU]

Regress residuals ε ̂ on spatially lagged residuals Wε ̂. Ideally, we would see that there is no relationship between ε ̂ and Wε ̂ – that is, that the coefficient of Wε ̂, which we denote by λ (as opposed to β1) is not significantly different from 0. (λ can range between -1 and 1). When we are dealing with the residual map that looks like the map on the left (in the maps on the previous page), the λ is close to 0; when we are dealing with a residual map that looks like the map on the right, the λ is close to 1.


### Testing Other Regression Assumptions in R
iii.	State that GeoDa, the tool that you’re using to run your OLS regression, also has a way of testing other regression assumptions. 

1.	The first is the assumption of homoscedasticity, which is tied to the assumption of independence of errors.
a.	State which test(s) is/are used to examine data for heteroscedasticity in GeoDa, and state the null and alternative hypotheses. =

We can use the following tests in GeoDa to test the null hypothesis of homoscedasticity, against the alternative hypothesis of heteroscedasticity.
i.	Koenker-Bassett Test
ii.	Breusch-Pagan Test
iii.	White Test
Ideally, we would want to fail to reject the null hypothesis for the alternative hypothesis (i.e., get a p-value > 0.05).


2.	Another assumption is that of normality of errors.
a.	State which test is used to test for normality of errors in GeoDa, and state the null and alternative hypotheses.

2.	When residuals are not normal, we may run into some problems for OLS (and spatial) regression. We can check for normality by:
a.	Examining the histogram of residuals
b.	Looking at the Jarque-Bera test in Geoda


##-----------------------------min doing-------------------------------------------------------------------------------------------##




## Spatial Lag and Spatial Error Regression

### Spatial Lag Regression
ii.	Describe the method of spatial lag regression in several sentences. 
1.	Present the model equation for the spatial lag model. 

In simpler terms, we can understand spatial lag through the analogy of this Stats Wizard Penguin (SWP) and his Inept Companions (ICs): although the SWP may have a stats wizardry quotient (SWQ) of over 9,000, its spatial lag (i.e., the average value of the SWQs of its neighboring ICs) is 0 because all the neighboring penguins know exactly 0 about statistics. (Crucially, we should use a symmetrical matrix of statistically incompetent penguins, as this is a key requirement of spatial lag and spatial error regressions).
```{r tweet embed, echo = F}
library(tweetrmd)

tweetrmd::tweet_embed("https://twitter.com/schumacherbj/status/1584278266546130944",
                      widget_type = "video")
```

a.	Instead of writing X1…X4, write the names of the actual predictors that you’re using in this assignment (e.g., PCTVACANT)
b.	Explain what each term is (the β coefficients, ρ, ε, etc)

### Spatial Error Regression
iii.	Describe the method of spatial error regression in several sentences. 
1.	Present the model equation for the spatial error model. 
a.	Instead of writing X1…X4, write the names of the actual predictors that you’re using in this assignment (e.g., PCTVACANT)
b.	Explain what each term is (the β coefficients, λ, ε, u, etc)

### Assumptions for Spatial Lag and Error Regression
Indicate that the assumptions that are needed for OLS are still needed for both spatial lag and spatial error regression models (except that of spatial independence of observat

### Goals of Spatial Lag and Error Regression
State the goal of spatial lag and spatial error regression (i.e., what you hope will happen with regression residuals as a result of using these methods). 

### Regression Performance Compared
vi.	Mention that you will compare the results of spatial lag regression with OLS and the results of spatial error regression with OLS, and will decide whether the spatial models perform better than OLS based a number of criteria. 
1.	These criteria include
a.	Akaike Information Criterion/Schwarz Criterion;
b.	Log Likelihood; 
c.	Likelihood Ratio Test
2.	Be sure to describe what each of the above criteria is, and how you decide which model is better based on this criterion (state any null/alternative hypotheses, if applicable).
3.	State that another way of comparing OLS results with spatial lag and spatial error results is by looking at the Moran’s I of regression residuals. 
a.	Indicate how you would decide which model is better based on this criterion.

## Geographically Weighted Regression

### Simpson's Paradox and Local Regression
Introduce GWR by talking about the concepts of Simpson’s paradox and local regression.

### GWR Equations
Present the GWR equations and explain them in your own words

### Running Local Regression
Talk about how local regression is run

### Bandwidth (Adaptive vs. Fixed)
Discuss the concept of bandwidth, and talk about adaptive vs. fixed bandwidth.
1.	State that here, you will be using adaptive bandwidth
a.	Explain why adaptive bandwidth is more appropriate in this problem than the fixed bandwidth

### Assumptions in GWR
Mention that the OLS assumptions still hold in GWR.
1.	When mentioning multicollinearity, talk about the Condition Number, and the issues of multicollinearity/clustering in GWR.

### GWR and P-Values
Indicate why p-values are not part of the GWR output.

## Tools
- List packages for spatial lag & spatial error
- List packages for GWR
```{r, include=F}

# install.packages(c("sp", "rgdal", "rgeos", "spdep", "spgwr", "spatialreg", "GWmodel"))
#install.packages("remotes")
#remotes::install_github("gadenbuie/tweetrmd")

#remove.packages("rlang")
#install.packages("rlang")

library(tidyverse) #general
library(sf) #spatial
library(mapview) #quick mapping
library(tmap) #full mapping
library(ggpubr) #for ggarrange
library(gt) #for tables
library(glue) #for tables
library(janitor) #to clean col names
library(corrplot) #for easy correlation matrix
library(tmap) #for choropleth maps
library(MASS) #for stepwise regression
library(DAAG) #for CVlm
library(caret) #for a different attempt at cvlm
library(stargazer) # for cleaner reg tables
library(sp) # for spatial
library(rgdal) # for spatial
library(rgeos) # for spatial
library(spdep) #for spatial lag + error reg
library(spgwr) # for geographically weighted reg
library(spatialreg) #for spatial lag + error reg (alt)
library(GWmodel) # for geographically weighted reg (alt)
library(tweetrmd) # to embed a tweet
library(sfdep) # for sf replacement of spdep
library(lmtest) #for breusch pagan test
library(whitestrap) # for white test
library(tseries) # for jarque bera test
```

## Import and Prepare Data

As in Homework #1, we are completing this assignment in R (rather than using GeoDa and ArcGIS). Once again, we will read in the original shapefile of data and transform the relevant columns to produce the initial data that we need for our analysis.

```{r import}
reg_data = st_read('C:/Users/Nissim/Desktop/Fall 2022/Spat Stats/musa-500-hmwk-2/reg_data_files/Regression Data.shp')

#Min
#reg_data = read_sf('C:/Users/vestalk/Desktop/00_Upenn/20.Fall/03.MUSA 5000 Spatial Statistics and Data Analysis/Assignment/HW1/RegressionData.shp')

#define a function to find zero values in columns

# This function applies log transformations to the relevant columns. It checks whether there are zero values in each column and then applies the appropriate log transformation accordingly.

col_zeros = function(a, b) {
                  pct_col_zeros = count(subset(st_drop_geometry(a), b != 0)) |>
                                      pull(n) / nrow(st_drop_geometry(a))
                  return(pct_col_zeros)
                  }
#apply function with case_when statement
#case_when is a vectorized function, while ifelse is not.
#running this with ifelse will result in all row values in the mutated column being the same.
reg_data = reg_data |>
            mutate(
                ln_med_h_val = case_when(col_zeros(reg_data, reg_data$MEDHVAL) == 1 ~ log(reg_data$MEDHVAL),
                                     TRUE ~ log(1 + reg_data$MEDHVAL)),
                   ln_pct_bach_more = case_when(col_zeros(reg_data, reg_data$PCTBACHMOR) == 1 ~ log(reg_data$PCTBACHMOR),
                                     TRUE ~ log(1 + reg_data$PCTBACHMOR)),
                   ln_n_bel_pov_100 = case_when(col_zeros(reg_data, reg_data$NBelPov100) == 1 ~ log(reg_data$NBelPov100),
                                     TRUE ~ log(1 + reg_data$NBelPov100)),
                   ln_pct_vacant = case_when(col_zeros(reg_data, reg_data$PCTVACANT) == 1 ~ log(reg_data$PCTVACANT),
                                     TRUE ~ log(1 + reg_data$PCTVACANT)),
                   ln_pct_singles = case_when(col_zeros(reg_data, reg_data$PCTSINGLES) == 1 ~ log(reg_data$PCTSINGLES),
                                     TRUE ~ log(1 + reg_data$PCTSINGLES)),
                  )
```


# Results

## Spatial Autocorrelation

### Distribution of Median House Value
The map below displays the spatial distribution of median house value by census tract in Philadelphia.
```{r medhval choro}
tmap_mode("plot")

tm_shape(reg_data) + 
  tm_polygons(title = "ln(MEDHVAL)", col = "LNMEDHVAL", border.col = NA, border.alpha = 0, lwd = 0, palette = "viridis", style = "jenks") + 
 # tm_shape(phl_city_lims) +
#  tm_borders(col = "grey", lwd = 5) +
  tm_compass(position = c("left", "top")) +
  tm_layout(main.title = "Median House Values by Tract",
            legend.position = c("right", "bottom"),
            frame = FALSE)
```


### Global Moran's I
Present and describe the global Moran’s I value and the random permutations test results.

Here, we'll prepare a weight matrix based on Queen neighbors.^[This analysis is based on [Data Analysis and Visualization with R:Spatial (2022)](http://www.geo.hunter.cuny.edu/~ssun/R-Spatial/)] We can do this using the `spdep` package; `spdep::poly2nd` produces neighboring relations and then `spdep::nb2listw` converts them to a weight matrix. 

Here we're using the `sfdep` package (rather than `spdep`), as we prefer working with `sf` objects to `sp` objects. Documentation is [available here](https://sfdep.josiahparry.com/).
```{r queen weights}

# create our queen weight matrix
# reg_data_nb = reg_data |>
        #              spdep::poly2nb(row.names = reg_data$POLY_ID)

# grab geometry
reg_data = reg_data |>
              mutate(nb_geom = st_geometry(reg_data), #geoms for poly2nb
                     nb = st_contiguity(nb_geom), # generate neighbors
                     weights = st_weights(nb)) # weight matrices from neighbors

summary(reg_data$nb)
```

We can visualize the connections between neighbors based on the weight matrix that we've connected. Here's what they look like:
```{r global moran map}

# install.packages("sfnetworks")
# install.packages("igraph")
# install.packages("tidygraph")
# remotes::install_github("luukvdmeer/sfnetworks", "fix-onattach")
# library(igraph)
# library(tidygraph)
# library(sfnet)

reg_data_lines = nb2lines(reg_data$nb, 
                          coords = st_centroid(reg_data$geometry), 
                          as_sf = TRUE)

tm_shape(reg_data) + 
  tm_borders(col = "grey", lwd = 0.5) + 
tm_shape(reg_data) +
  tm_dots() +
tm_shape(reg_data_lines) +
  tm_lines(col = "red") +
tm_layout(frame = FALSE)
```

Now we can apply the weight matrix that to calculate the global Moran's I.
```{r global moran}
# Global Moran's I

# now use that weight matrix to calculate global moran's i for MEDHVAL

global_moran(reg_data$LNMEDINC, reg_data$nb, reg_data$weights)$`I`
```

Having calculated a global Moran's I, we'll use a Monte Carlo approach to test whether our global Moran's I is statistically significant. If the null hypothesis is true, then the probability of drawing the observed data is the same as any other permutation of the zi’s amongst the polygons. Thus, if m just the number if simulated Moran’s-I values exceeding the observed one, and M is the total number of simulations, then the probability of getting the observed Moran’s-I

or a greater one is

$p = m+1 / M+1$

```{r global moran test}

# reshuffling test w 999 permutations
# present Moran's I value, histogram of Moran's I values with all permutations, and p-value

moranMC = global_moran_perm(reg_data$LNMEDINC, reg_data$nb, reg_data$weights, alternative = "two.sided", 999)

moranMC
```

Here we visualize the randomly permuted values from the Monte Carlo test. We can see that the calculated global Moran's I is far higher than the distribution of the random values. In fact, it is the highest (observed rank = 1000). It's p-value is < 0.001 and we can therefore reject the null hypothesis.
```{r global moran hist}
#draws a histogram of our randomly permuted values
# and adds a red line for our actual moran's i

moranMCres = moranMC$res |>
              as.data.frame()

colnames(moranMCres) = "col1"

ggplot(moranMCres) +
  geom_histogram(aes(col1), bins = 100) +
  geom_vline(xintercept = moranMC$statistic, col = "red") +
  labs(title = "Histogram of MoranMCres",
       x = "moranMCres",
       y = "Frequency") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

Additional confirmation of the significance of our Moran's I value can be found by plotting the relationship between LNMEDINC of the block groups and their neighbors. If there were no relationship between block group observations and those of their neighbors, we would not see a clear pattern in the plot below. However, we observe that this is not the case.
```{r global moran plot}
moran.plot(reg_data$LNMEDINC, nb2listw(reg_data$nb),
           xlab = "ln(MEDHINC)", 
            ylab = "Spatial Lag")
```

### Local Moran's I
The `local_moran` function in `sfdep` allows us to compute LISA statistics for each block group. This function generates the Local Moran statistic as `Ii`. We might also be interested in `Var.Ii`, which tells us how much each block group’s Local Moran Statistic varies from the global mean, and $Pr(z>0)$, which indicates whether the Local Moran Statistic can be considered statistically significant.

For Local Moran’s I results, present the Significance Map and Cluster Map obtained by running the Local Morans’ I. 
1.	Discuss the results: what are the not significant, high-high, high-low, low-high and low-low areas on the Cluster Map? Where in the city are these areas? 
```{r local moran}
# Local Moran's I

# local Moran's I analysis with queen weight matrix; print results

lisa = local_moran(reg_data$LNMEDINC, reg_data$nb, reg_data$weights, nsim = 999)

head(lisa)
```

Now we can examine the local Moran's I values to see if there is significant spatial autocorrelation.
```{r local moran map}
# first we need to bind the lisa dataframe to our reg_data dataframe

reg_data = cbind(reg_data, lisa)

tm_shape(reg_data) +
  tm_fill(style = "fixed", 
          col = "p_ii", 
          breaks = c(0,0.001, 0.01, 0.05, 1), 
          palette = '-viridis',
          title = "P-Value") +
  #tm_borders(col = "white",
   #          lwd = 0.05) +
  tm_layout(frame = FALSE, main.title = "P-Value")
```

We can also visualize whether the LISA statistics suggest high or low clustering. [Ugh, fix this wording plz.]

```{r local moran clustering}
reg_data = reg_data |>
            mutate(
                  mean = as.character(mean),
                  lisa_map = case_when(
                                  p_ii < 0.05 ~ mean,
                                  TRUE ~ "Insignificant"
                                )
                  )

pal = c("red", "pink", "lightgrey", "lightblue", "blue")

tm_shape(reg_data) +
  tm_fill(col = "lisa_map", palette = pal, title = "LISA Clusters")+
  tm_borders(col = "white", lwd = 0.05) +
  tm_layout(frame = FALSE, main.title = "LISA Clusters")

```

## OLS Regression and Assumptions

### OLS Output
i.	Present the OLS output from GeoDa (call this Table 1)
1.	Give a brief 2 sentence overview of the OLS results (feel free to paste this from your description in HW 1). That is, simply indicate which predictors are significant and what % of variance in LNMEDHVAL has been explained by the model. 
2.	Comment on the results of the tests on heteroscedasticity
a.	Are the results from the 3 tests consistent with each other? 
b.	Do they indicate a problem with heteroscedasticity?
3.	Comment on the results of the test on normality of errors
a.	Do test results indicate a problem with normality?
```{r ols}
# OLS Regression

reg = lm(formula=LNMEDINC ~ LNMEDHVAL + PCTVACANT, data = reg_data)

summary(reg)
```
Here we run a log likelihood test.
```{r log liklihood}
logLik(reg)
```
To test for heteroscedasticity, we will run three tests: 

 1) the Breusch-Pagan test
```{r bp test}
bptest(reg, studentize = FALSE)
```

 2) the Koenker-Bassett test (also known as the Studentized Breusch-Pagan test)
```{r kb test}
bptest(reg, studentize = TRUE)
```

 3) the White test
```{r white test}
white_test(reg)
```

Finally, we will run the Jarque-Bera test to assess whether the residuals of our regression are normal.
```{r jb test}
jarque.bera.test(reg$residuals)
```

### Scatterplot of Residuals
ii.	Present the scatterplot of OLS_RESIDU by WT_RESIDU and describe the results.
1.	Are the results (based on the value and significance level of ρ – that’s referred to as Slope b in the results) indicative of significant spatial autocorrelation?

Now, let’s generate standardized residuals, which are OLS Model residuals divided by an estimate of their standard deviation, and map them. Visually, it certainly seems that there’s spatial autocorrelation in the residuals, with some higher values clustered in the northeast and northwest of the city, and some lower values clustered in north Philadelphia and downtown.
```{r ols resids}
reg_data = reg_data |>
              mutate(stand_resids = rstandard(reg),
                     spatial_lag = st_lag(stand_resids, 
                                             nb, 
                                             weights))

tm_shape(reg_data)+
  tm_fill(col = 'stand_resids', 
          style = 'quantile', 
          title = 'Standardized OLS Residuals', 
          palette ='Blues')+
  tm_layout(frame = FALSE, 
            title = 'Standardised OLS Residuals')
```
However, a visual assessment is not sufficient, and we will test the presence of spatial autocorrelation in two ways: 1) by regressing residuals on their queen neighbors, and 2) by looking at the Moran’s I of the residuals.

First, let’s regress the OLS standardized residuals on the spatial lag of the OLS residuals (i.e., OLS residuals at the queen neighbors). We can see that the beta coefficient of the lagged residuals (sometimes also referred to as rho or lambda) is significant and positive (0.598, p<0.0001), meaning that there’s a significant level of spatial autocorrelation in the residuals. This is consistent with Moran’s I of the residuals we see below.
```{r reg resids on nn}
resids_lm = lm(formula = stand_resids ~ spatial_lag, data = reg_data)

summary(resids_lm)
```
### Moran's I Significance

Once again, we can use the Monte Carlo simulation to generate a Moran's I statistics and a pseudo p-value.
```{r stand resids mc}
global_moran_perm(reg_data$stand_resids, reg_data$nb, reg_data$weights, alternative = "two.sided", 999)
```
It is strongly apparent that spatial autocorrelation exists among the regression residuals of the OLS Model. The p-value is very small indicating that Moran’s I is significant. Because there’s clearly spatial autocorrelation in OLS residuals, the OLS Model is inappropriate and we need to consider another method. Here, we will attempt to run the Spatial Lag Model, the Spatial Error Model, and Geographically Weighted Regression.
```{r stand resids scatter}
moran.plot(reg_data$stand_resids, nb2listw(reg_data$nb),
           xlab = "Standardized Residuals", 
            ylab = "Spatial Lag")
```


## Spatial Lag and Spatial Error Regression

### Spatial Lag Regression Results
i.	Present results of Spatial Lag regression (call this Table 2)
1.	Talk about the W_LNMEDHVAL term in the spatial lag regression output. State whether it is significant, and how the results can be interpreted.
2.	Are the remaining terms (i.e., the predictors LNNBELPOV, PCTBACHMOR, PCTSINGLES, and PCTVACANT) in the model significant? 
a.	Compare these results to OLS results.
3.	State whether, based on the Breusch-Pagan test, the spatial lag regression residuals are still heteroscedastic.
4.	Compare the Spatial Lag regression and OLS regression models based on the Akaike Information Criterion/Schwarz Criterion, the Log Likelihood, and the Likelihood Ratio Test. 

```{r spatial lag}
spatial_lag_reg = lagsarlm(formula = LNMEDINC ~ LNMEDHVAL + PCTVACANT, data = reg_data, nb2listw(reg_data$nb))

summary(spatial_lag_reg)
```
As with our OLS before, we will calculate the likelihood ratio for our spatial lag model.
```{r likelihood ratio}
LR.Sarlm(spatial_lag_reg, reg) #Here lagreg is the SL output; reg is the OLS output
```

To test for heteroscedasticity, we will once again run two of the three tests run above: 

 1) the Breusch-Pagan test
```{r bp test lag reg}
bptest.Sarlm(spatial_lag_reg, studentize = FALSE)
```

 2) the Koenker-Bassett test (also known as the Studentized Breusch-Pagan test)
```{r kb test lag reg}
bptest.Sarlm(spatial_lag_reg, studentize = TRUE)
```

And, again, the Jarque-Bera test to assess whether the residuals of our regression are normal.
```{r jb test lag reg}
jarque.bera.test(spatial_lag_reg$residuals)
```

```{r spatial lag stnad resids map}
reg_data = reg_data |>
              mutate(
                spatial_lag_resids = lagsarlm(formula = LNMEDINC ~ LNMEDHVAL + PCTVACANT, data = reg_data, nb2listw(reg_data$nb))$residuals
              )

tm_shape(reg_data)+
  tm_fill(col = 'spatial_lag_resids', 
          style = 'quantile', 
          title = 'Spatial Lag Residuals', 
          palette ='Blues')+
  tm_layout(frame = FALSE)
```

```{r spatial lag stand resids mc}
global_moran_perm(spatial_lag_reg$residuals, reg_data$nb, reg_data$weights, alternative = "two.sided", 999)
```


5.	Present the Moran’s I scatterplot of spatial lag regression residuals. Does there seem to be less spatial autocorrelation in these residuals than in OLS residuals?
6.	Overall, which model is doing better based on all of these criteria?
```{r spatial lag moran}
# Moran's I for spatial lag model

# once again, run moran's I for the spatial lag residuals; test w 999 permutations
# print scatterplot & summary table of significance test
```

### Spatial Error Regression Results
Present results of Spatial Error regression (call this Table 3)
1.	Talk about the LAMBDA term in the spatial lag regression output. State whether it is significant, and how the results can be interpreted.
2.	Are the remaining terms (i.e., the predictors LNNBELPOV, PCTBACHMOR, PCTSINGLES, and PCTVACANT) in the model significant? 
a.	Compare these results to OLS results.
3.	State whether, based on the Breusch-Pagan test, the spatial lag regression residuals are still heteroscedastic? 
4.	Compare the Spatial Error regression and OLS regression based on the Akaike Information Criterion/Schwarz Criterion, the Log Likelihood, and the Likelihood Ratio Test. 

```{r spatial error}
# Spatial Error model
# using queen weights again, calculate spatial error for med_h_val
# save residuals in a new column
# also save predicted value 
```

5.	Present the Moran’s I scatterplot of spatial error regression residuals. Does there seem to be less spatial autocorrelation in these residuals than in OLS residuals?
6.	Overall, which model is doing better based on all of these criteria?
```{r spatial error moran}
# Moran's I for spatial error model

# once again, run moran's I for the spatial error residuals; test w 999 permutations
# print scatterplot & summary table of significance test
```

### Spatial Lag vs. Spatial Error
Compare the Spatial Lag and Spatial Error results with each other
1.	Recall that you should not be using the likelihood-ratio test for this because the models are not nested (i.e., neither method is a special subtype of each other). However, it is OK to compare the two non-nested models, such as spatial lag and spatial error, based on Akaike Information Criterion and the Schwarz Information Criterion.
a.	Which model has better (lower) Akaike Information Criterion and Schwarz Information Criterion values?

## Geographically Weighted Regression

### GWR Results
i.	Present GWR results from the _supp table (call this Table 4)
1.	Compare the (overall) R-squared of the GWR regression with the R-squared of the OLS regression. State which regression method seems to be doing a better job of explaining the variance in the dependent variable.
2.	Compare the Akaike Information Criteria of GWR with those of OLS, Spatial Lag and Spatial Error models. Which model seems to be doing a better job based on that (remember, the lower the Akaike Information Criterion, the better the fit).
```{r gwr}
# Geographically Weighted Regression

# run geographically weighted regression model uising:
  # lnnbelpov
  # pctbachmor
  # pctsingles
  # pctvacant
# make sure to produce results as shapefile
# kernel type should be "adaptive"; bandwidth type should be "AICc"
# print out:
  # supplemental table
  # choropleth map of local R-squared values
```

### Moran's I Scatterplot of GWR Residuals
ii.	Present the Moran’s I scatterplot of GWR residuals. Does there seem to be less spatial autocorrelation in these residuals than in OLS residuals? What about the Spatial Lag and Spatial Error Residuals.
```{r gwr moran}
# Moran's I of GWR Residuals

# once again, run moran's I for the GWR residuals; test w 999 permutations
# print scatterplot & summary table of significance test
```

### Local Regression Results
iii.	Be sure to discuss local regression results, as is done on the slides. Are there locations in the city where the relationships between each of the predictors and the dependent variable possibly significant?
```{r local reg}
# Local Regression Results

# Follow instructions from class slides to obtain local regression results. Specifically, present maps of the ratio of the beta coefficients and the standard error estimates.
# Use dark red when the ratio is < -2, pink when the ratio is between 0 and -2, light blue when the ratio is between 0 and 2, and dark blue when the ratio is > 2.
```

### Local R-Squared Results
iv.	Present and discuss a choropleth map of local R-squared results. 

# Discussion
a)	In a couple sentences, recap what you did in the paper and your findings. Discuss what conclusions you can draw, and which of the four regression methods (OLS, Spatial Lag, Spatial Error, GWR) was the best, based on the results. 
b)	Give a brief description of the limitations (i.e., which assumptions were not met).

# Citations
```{r citation lolz}
url = "https://www.smbc-comics.com/comics/1618931828-20210420.png"
```
<center><img src="`r url`"></center>

